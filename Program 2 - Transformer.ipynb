{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation (WSD)\n",
    "### Sam Timmins, Alex Cerpa, Kas Taghavi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove_quotes(line):\n",
    "    # Remove starting and ending quotes.\n",
    "    if line.startswith('\"'):\n",
    "        line = line[1:]\n",
    "    if line.endswith('\"'):\n",
    "        line = line[:-1]\n",
    "    return line\n",
    "\n",
    "def preprocess_sentence(s, word, senses):\n",
    "    # Add the word and its senses to the input for the model.\n",
    "    s += f' [SEP] {word}'\n",
    "    for sense in senses:\n",
    "        s += f' [SEP] {sense}'\n",
    "    return s\n",
    "\n",
    "\n",
    "def parse_file_to_df(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = [remove_quotes(line.strip()) for line in f.readlines()]\n",
    "        word = lines[0]\n",
    "        senses = []\n",
    "        \n",
    "        # Read senses\n",
    "        i = 2\n",
    "        for i in range(2, len(lines)):\n",
    "            if not re.search(r'^[0-9]:? \\([a-z]+\\)', lines[i]):\n",
    "                break\n",
    "            else:\n",
    "                sense_line = lines[i]\n",
    "                sense_line = sense_line.replace('(', '').replace(')', '')\n",
    "                senses.append(sense_line)\n",
    "        \n",
    "        curr_sense = 1\n",
    "        sentences = []\n",
    "        sense = []\n",
    "        # Read sentences\n",
    "        for i in range(i, len(lines)):\n",
    "            if not lines[i]:\n",
    "                continue\n",
    "            if re.match(r'[0-9]', lines[i]):\n",
    "                curr_sense = int(lines[i])\n",
    "            else:\n",
    "                s = lines[i].strip()\n",
    "                sentences.append(preprocess_sentence(s, word ,senses))\n",
    "                sense.append(curr_sense - 1)\n",
    "            \n",
    "        \n",
    "        \n",
    "        return senses, pd.DataFrame({\"sentence\": sentences, \"sense\": sense})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubbish_senses, rubbish_df = parse_file_to_df('rubbish.txt')\n",
    "tissue_senses, tissue_df = parse_file_to_df('tissue.txt')\n",
    "yarn_senses, yarn_df = parse_file_to_df('yarn.txt')\n",
    "\n",
    "words = ['rubbish', 'tissue', 'yarn']\n",
    "dfs = [rubbish_df, tissue_df, yarn_df]\n",
    "\n",
    "for df in dfs:\n",
    "    df['sentence'] = df['sentence'].str.replace('[^\\w\\s\\[\\]]','', regex=True)\n",
    "    df['sentence'] = df['sentence'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The yarn is no longer novel  too many other wr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have just finished reading a rather lengthy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But spin those reporters some yarn and keep th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They censored all that out of my copy  made it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nan do you think that kind of yarn is going to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>The yarn connected people across lands and oce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>It was a reminder that no matter where we come...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>In the end the humble yarn held within it the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>It was a testament to the strength of communit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>And as the yarn continued to weave its way thr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  sense\n",
       "0    The yarn is no longer novel  too many other wr...      0\n",
       "1    I have just finished reading a rather lengthy ...      0\n",
       "2    But spin those reporters some yarn and keep th...      0\n",
       "3    They censored all that out of my copy  made it...      0\n",
       "4    Nan do you think that kind of yarn is going to...      0\n",
       "..                                                 ...    ...\n",
       "104  The yarn connected people across lands and oce...      1\n",
       "105  It was a reminder that no matter where we come...      1\n",
       "106  In the end the humble yarn held within it the ...      1\n",
       "107  It was a testament to the strength of communit...      1\n",
       "108  And as the yarn continued to weave its way thr...      1\n",
       "\n",
       "[109 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yarn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The yarn is no longer novel  too many other writers have since taken off from Galluns inspiration  but it is just as fine to me as it always was [SEP] Yarn [SEP] 1 n narration recital yarn the act of giving an account describing incidents or a course of events his narration was hesitant [SEP] 2 n thread yarn a fine cord of twisted fibers of cotton or silk or wool or nylon etc used in sewing and weaving'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[2].iloc[0]['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rubbish': (<__main__.WordSenseDataset at 0x7fe5a226c8e0>,\n",
       "  <__main__.WordSenseDataset at 0x7fe5a226c820>),\n",
       " 'tissue': (<__main__.WordSenseDataset at 0x7fe59193a520>,\n",
       "  <__main__.WordSenseDataset at 0x7fe5a226bd30>),\n",
       " 'yarn': (<__main__.WordSenseDataset at 0x7fe5a226bc10>,\n",
       "  <__main__.WordSenseDataset at 0x7fe5a226bb50>)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WordSenseDataset(Dataset):\n",
    "    def __init__(self, encodings, senses):\n",
    "        self.encodings = encodings\n",
    "        self.labels = senses\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels) \n",
    "\n",
    "\n",
    "def encode(texts, tokenizer, max_length=512):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=False, max_length=max_length, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "\n",
    "for word, df in zip(words, dfs):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['sentence'].to_numpy(), df['sense'].to_numpy(), test_size=0.1)\n",
    "\n",
    "    train_encodings = encode(X_train.tolist(), tokenizer=tokenizer)\n",
    "    test_encodings = encode(X_test.tolist(), tokenizer=tokenizer)\n",
    "    \n",
    "    train_dataset = WordSenseDataset(train_encodings, y_train)\n",
    "    test_dataset = WordSenseDataset(test_encodings, y_test)\n",
    "    datasets[word] = (train_dataset, test_dataset)\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 86.3 ms, sys: 29.6 ms, total: 116 ms\n",
      "Wall time: 389 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Training model for yarn:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/envs/article-bias/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 03:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.682600</td>\n",
       "      <td>0.644054</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.647700</td>\n",
       "      <td>0.555471</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.456069</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for word, (train_dataset, test_dataset) in datasets.items():\n",
    "    # if word != 'yarn': continue\n",
    "    print(f'Training model for {word}:')\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(f'{word}-word-sense')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model_name = 'yarn-word-sense'\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "pipe = pipeline(\"text-classification\", \n",
    "                model=model_name, \n",
    "                tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 n narration, recital, yarn the act of giving an account describing incidents or a course of events \"his narration was hesitant', '2 n thread, yarn a fine cord of twisted fibers of cotton or silk or wool or nylon etc. used in sewing and weaving']\n"
     ]
    }
   ],
   "source": [
    "print(yarn_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Her yarn about the enchanted forest captured the imagination of the listeners. [SEP] yarn [SEP] 1 n narration, recital, yarn the act of giving an account describing incidents or a course of events \"his narration was hesitant [SEP] 2 n thread, yarn a fine cord of twisted fibers of cotton or silk or wool or nylon etc. used in sewing and weaving\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.7731480598449707}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = preprocess_sentence(\n",
    "    'Her yarn about the enchanted forest captured the imagination of the listeners.',\n",
    "    'yarn', yarn_senses)\n",
    "print(input_)\n",
    "pipe(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sailor's yarn of his journey across the sea had everyone on the edge of their seats. [SEP] yarn [SEP] 1 n narration, recital, yarn the act of giving an account describing incidents or a course of events \"his narration was hesitant [SEP] 2 n thread, yarn a fine cord of twisted fibers of cotton or silk or wool or nylon etc. used in sewing and weaving\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.6420883536338806}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = preprocess_sentence(\n",
    "    'The sailor\\'s yarn of his journey across the sea had everyone on the edge of their seats.',\n",
    "    'yarn', yarn_senses)\n",
    "print(input_)\n",
    "pipe(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't stand listening to the lecturer's rubbish for another minute and walked out of the lecture hall. [SEP] rubbish [SEP] 1: n rubbish, trash, scrap worthless material that is to be disposed of [SEP] 2: n folderol, rubbish, tripe, trumpery, trash, wish-wash, applesauce, codswallop nonsensical talk or writing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5794568061828613}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = preprocess_sentence(\n",
    "    \"I couldn't stand listening to the lecturer's rubbish for another minute and walked out of the lecture hall.\",\n",
    "    'rubbish', rubbish_senses)\n",
    "print(input_)\n",
    "pipe(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.7427711486816406}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(preprocess_sentence(\n",
    "    'The construction site was littered with rubbish, including scraps of metal and discarded building materials.',\n",
    "    'rubbish', rubbish_senses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.5107261538505554}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(preprocess_sentence(\n",
    "    'Her yarn about the enchanted forest captured the imagination of the listeners.',\n",
    "    'yarn', yarn_senses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.34677550196647644,\n",
       " 'eval_accuracy': 0.9090909090909091,\n",
       " 'eval_runtime': 2.9576,\n",
       " 'eval_samples_per_second': 3.719,\n",
       " 'eval_steps_per_second': 0.676,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
